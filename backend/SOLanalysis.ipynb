{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6855cc5d-b575-4f02-8732-ff3a50708000",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mongopy in /opt/anaconda3/lib/python3.12/site-packages (0.1)\n",
      "Requirement already satisfied: nose in /opt/anaconda3/lib/python3.12/site-packages (from mongopy) (1.3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mongopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b09642a-4358-4add-9a0a-12af3152c30d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /opt/anaconda3/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (3.10.5)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (5.9.8)\n",
      "Requirement already satisfied: pyparsing in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (4.66.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (2024.12.14)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ea12e36-1dc3-41d3-9e9a-9b65e323c0be",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"solana_tx_db\"]\n",
    "collection = db[\"transactions\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7dab49-6815-4c7b-9204-c5463d3efd60",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        _id  \\\n",
      "0  687f2c646e21c4e4792e1119   \n",
      "1  687f2c646e21c4e4792e111a   \n",
      "2  687f2c646e21c4e4792e111b   \n",
      "3  687f2c646e21c4e4792e111c   \n",
      "4  687f2c646e21c4e4792e111d   \n",
      "\n",
      "                                           signature       slot   blockTime  \\\n",
      "0  4eJrP55btsYsWLGYSU6URG2y14uQiFDfRjL4TpezeWPUng...  354935068  1753164885   \n",
      "1  45mKgq7Yt1wifqGWbMwmkirAEkcNFUWfNaUEvCdUED5xTp...  354935068  1753164885   \n",
      "2  4kP2Q5FqWHbpDHxsPSEKzSecGCG35eNHHuD2jHcvwjigS6...  354935068  1753164885   \n",
      "3  gueBD215eV125kqLQ85eJjdHJ3ghTtFypvi5yJNfRxh4MS...  354935068  1753164885   \n",
      "4  2ZEsXfeWghSwZ7RVktH9zTLWqjaLQZYKXBcdAx3jCVCV6T...  354935068  1753164885   \n",
      "\n",
      "    status     type       fee     value  \\\n",
      "0  success  unknown  0.000005  0.000005   \n",
      "1  success  unknown  0.000005  0.000005   \n",
      "2  success  unknown  0.000005  0.000005   \n",
      "3  success  unknown  0.000005  0.000005   \n",
      "4  success  unknown  0.000005  0.000005   \n",
      "\n",
      "                                        programs  \\\n",
      "0  [Vote111111111111111111111111111111111111111]   \n",
      "1  [Vote111111111111111111111111111111111111111]   \n",
      "2  [Vote111111111111111111111111111111111111111]   \n",
      "3  [Vote111111111111111111111111111111111111111]   \n",
      "4  [Vote111111111111111111111111111111111111111]   \n",
      "\n",
      "                                         accountKeys  instructionCount   err  \n",
      "0  [ANC1u9sY36q3mi2MyVhtz71un8yLgTsFBUuyLcSPzKsk,...                 1  None  \n",
      "1  [BjuD62v9RysrburpKb65UKeaAWRSFyi7pFLLxdE3dPv, ...                 1  None  \n",
      "2  [DBfh9QUCZde2U6buTbNhxSEnaqPV9Z2cfmjsBJu9qrLw,...                 1  None  \n",
      "3  [AvNsK6uxBBwejyPe7tZqgX4onaCnXTqKQvKRaTe9Ekya,...                 1  None  \n",
      "4  [MEGAuHf4j7vBVGvM9m4vUjwfeJ7abCKbihWAcu3o2nY, ...                 1  None  \n"
     ]
    }
   ],
   "source": [
    "cursor = collection.find().limit(100)\n",
    "df = pd.DataFrame(list(cursor))\n",
    "\n",
    "\n",
    "if \"block_time\" in df.columns:\n",
    "    df[\"block_time\"] = pd.to_datetime(df[\"block_time\"])\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3841062d-ee93-4c27-b903-59688e0bc2f3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf956e0-6949-456f-bd83-f986a0401b3f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"This is to load data into dataframe so that it can be used by the graphs\"\"\"\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "def load_transactions(mongo_uri=\"mongodb://localhost:27017/\",\n",
    "                      db_name=\"solana_tx_db\", collection_name=\"transactions\"):\n",
    "    client = MongoClient(mongo_uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    cursor = collection.find({})\n",
    "    docs = list(cursor)\n",
    "    df = pd.DataFrame(docs)\n",
    "\n",
    "    # Ensure numeric types\n",
    "    df[\"fee\"] = pd.to_numeric(df[\"fee\"], errors=\"coerce\").fillna(0)\n",
    "    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\").fillna(0)\n",
    "    df[\"instructionCount\"] = pd.to_numeric(df[\"instructionCount\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "#Inorder to derive block level statistics we need to aggregate the data present in each of the blocks.\n",
    "def build_block_stats(df):\n",
    "    # Group by slot\n",
    "    block_stats = df.groupby(\"slot\").agg(\n",
    "        tx_count=(\"signature\", \"count\"),\n",
    "        success_count=(\"status\", lambda x: (x == \"success\").sum()),\n",
    "        fail_count=(\"status\", lambda x: (x != \"success\").sum()),\n",
    "        avg_fee=(\"fee\", \"mean\"),\n",
    "        median_fee=(\"fee\", \"median\"),\n",
    "        fee_variance=(\"fee\", \"var\"),\n",
    "        avg_value=(\"value\", \"mean\"),\n",
    "        median_value=(\"value\", \"median\"),\n",
    "        instruction_sum=(\"instructionCount\", \"sum\"),\n",
    "        block_time=(\"blockTime\", \"first\")\n",
    "    ).reset_index()\n",
    "\n",
    "    # Computing block time delta i.e gap between blocks\n",
    "    block_stats = block_stats.sort_values(\"slot\").reset_index(drop=True)\n",
    "    block_stats[\"block_time_delta\"] = block_stats[\"block_time\"].diff().fillna(0)\n",
    "    block_stats[\"block_utilization\"] = block_stats[\"tx_count\"] / block_stats[\"tx_count\"].max()\n",
    "\n",
    "    return block_stats\n",
    "\n",
    "#Building transactions level GNN using already present individual transactions.\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def build_transaction_graph(df):\n",
    "    features = df[[\"fee\", \"value\", \"instructionCount\"]].fillna(0)\n",
    "    scaler = StandardScaler()\n",
    "    x = torch.tensor(scaler.fit_transform(features), dtype=torch.float)\n",
    "\n",
    "    #Edges used to connect tx in same block (slot)\n",
    "    slot_groups = df.groupby(\"slot\").groups\n",
    "    src, dst = [], []\n",
    "    for txs in slot_groups.values():\n",
    "        txs = list(txs)\n",
    "        for i in range(len(txs)-1):\n",
    "            src.append(txs[i]); dst.append(txs[i+1])\n",
    "\n",
    "    edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n",
    "    y = torch.tensor((df[\"status\"] == \"success\").astype(int).values, dtype=torch.long)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "#Building block level GNN using already coomputed aggregated block stats\n",
    "def build_block_graph(block_stats):\n",
    "    features = block_stats[[\n",
    "        \"tx_count\", \"success_count\", \"fail_count\",\n",
    "        \"avg_fee\", \"median_fee\", \"fee_variance\",\n",
    "        \"block_time_delta\", \"block_utilization\"\n",
    "    ]].fillna(0)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x = torch.tensor(scaler.fit_transform(features), dtype=torch.float)\n",
    "    src = list(range(len(block_stats)-1))\n",
    "    dst = list(range(1, len(block_stats)))\n",
    "    edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n",
    "\n",
    "    # Labels: congestion = utilization > 0.8\n",
    "    y = torch.tensor((block_stats[\"block_utilization\"] > 0.8).astype(int).values, dtype=torch.long)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "\n",
    "# ----- Block Graph Encoder -----\n",
    "class BlockEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(BlockEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = F.relu(self.conv1(x, edge_index))\n",
    "        h = F.relu(self.conv2(h, edge_index))   # <-- added ReLU for stability\n",
    "        return global_mean_pool(h, batch)       # pooled block-level embedding\n",
    "\n",
    "\n",
    "# ----- Transaction Graph Encoder -----\n",
    "class TxEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(TxEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = F.relu(self.conv1(x, edge_index))\n",
    "        h = F.relu(self.conv2(h, edge_index))   # <-- added ReLU\n",
    "        return global_mean_pool(h, batch)       # pooled tx-level embedding\n",
    "\n",
    "\n",
    "# ----- Hybrid Congestion Prediction Model -----\n",
    "class HybridCongestionModel(nn.Module):\n",
    "    def __init__(self, block_in, tx_in, hidden=64, out_dim=2):\n",
    "        super(HybridCongestionModel, self).__init__()\n",
    "        # Encoders\n",
    "        self.block_encoder = BlockEncoder(block_in, hidden, hidden)\n",
    "        self.tx_encoder = TxEncoder(tx_in, hidden, hidden)\n",
    "        \n",
    "        # Fusion + classification\n",
    "        self.fc1 = nn.Linear(hidden*2, hidden)\n",
    "        self.dropout = nn.Dropout(0.3)            # regularization\n",
    "        self.fc2 = nn.Linear(hidden, out_dim)     # binary: congested / not congested\n",
    "    \n",
    "    def forward(self, block_data, tx_data):\n",
    "        block_emb = self.block_encoder(block_data.x, block_data.edge_index, block_data.batch)\n",
    "        tx_emb = self.tx_encoder(tx_data.x, tx_data.edge_index, tx_data.batch)\n",
    "        \n",
    "        # Fuse embeddings\n",
    "        h = torch.cat([block_emb, tx_emb], dim=1)  # [batch_size, hidden*2]\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = self.dropout(h)\n",
    "        return self.fc2(h)   # logits (use CrossEntropyLoss)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# ----- 1. Dataset Splitting -----\n",
    "def split_dataset(block_graphs, tx_graphs, labels, test_size=0.2):\n",
    "    # Ensure block_graphs[i] matches tx_graphs[i] and labels[i]\n",
    "    idx_train, idx_test = train_test_split(range(len(labels)), test_size=test_size, random_state=42, stratify=labels)\n",
    "\n",
    "    train_blocks = [block_graphs[i] for i in idx_train]\n",
    "    test_blocks  = [block_graphs[i] for i in idx_test]\n",
    "    train_txs    = [tx_graphs[i] for i in idx_train]\n",
    "    test_txs     = [tx_graphs[i] for i in idx_test]\n",
    "    y_train      = labels[idx_train]\n",
    "    y_test       = labels[idx_test]\n",
    "\n",
    "    return train_blocks, test_blocks, train_txs, test_txs, y_train, y_test\n",
    "\n",
    "\n",
    "# ----- 2. Custom Dataloader Wrapper -----\n",
    "class HybridDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, block_graphs, tx_graphs, labels):\n",
    "        self.block_graphs = block_graphs\n",
    "        self.tx_graphs = tx_graphs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.block_graphs[idx], self.tx_graphs[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# ----- 3. Collate Function for Hybrid Graphs -----\n",
    "def collate_fn(batch):\n",
    "    block_batch, tx_batch, labels = zip(*batch)\n",
    "    return (\n",
    "        torch_geometric.data.Batch.from_data_list(block_batch),\n",
    "        torch_geometric.data.Batch.from_data_list(tx_batch),\n",
    "        torch.tensor(labels, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "\n",
    "# ----- 4. Training Function -----\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for block_data, tx_data, labels in loader:\n",
    "        block_data, tx_data, labels = block_data.to(device), tx_data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(block_data, tx_data)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = out.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "# ----- 5. Evaluation Function -----\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for block_data, tx_data, labels in loader:\n",
    "            block_data, tx_data, labels = block_data.to(device), tx_data.to(device), labels.to(device)\n",
    "\n",
    "            out = model(block_data, tx_data)\n",
    "            loss = criterion(out, labels)\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "# ----- 6. Main Training Loop -----\n",
    "def run_training(block_graphs, tx_graphs, labels, model, epochs=20, batch_size=32, lr=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Split dataset\n",
    "    train_blocks, test_blocks, train_txs, test_txs, y_train, y_test = split_dataset(block_graphs, tx_graphs, labels)\n",
    "\n",
    "    # Create datasets & loaders\n",
    "    train_dataset = HybridDataset(train_blocks, train_txs, y_train)\n",
    "    test_dataset  = HybridDataset(test_blocks, test_txs, y_test)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Optimizer & Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}: \"\n",
    "              f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n",
    "              f\"Test Loss={test_loss:.4f}, Test Acc={test_acc:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777692f4-092a-45d9-b625-52fa67223efd",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb76396-3e9b-42ee-a476-20b00e7060c9",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51857fe3-7d7b-4721-8846-9f82a4c5e9d9",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e5934732-fa3f-4008-99d0-621e4b2c6315",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"This is to load data into dataframe so that it can be used by the graphs\"\"\"\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "def load_transactions(mongo_uri=\"mongodb://localhost:27017/\",\n",
    "                      db_name=\"solana_tx_db\", collection_name=\"transactions\"):\n",
    "    client = MongoClient(mongo_uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    cursor = collection.find({})\n",
    "    docs = list(cursor)\n",
    "    df = pd.DataFrame(docs)\n",
    "\n",
    "    # Ensure numeric types\n",
    "    df[\"fee\"] = pd.to_numeric(df[\"fee\"], errors=\"coerce\").fillna(0)\n",
    "    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\").fillna(0)\n",
    "    df[\"instructionCount\"] = pd.to_numeric(df[\"instructionCount\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3c01ead0-da44-42da-a9d3-8c0531efd143",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#Inorder to derive block level statistics we need to aggregate the data present in each of the blocks.\n",
    "def build_block_stats(df):\n",
    "    #Grouping blocks by slot\n",
    "    block_stats = df.groupby(\"slot\").agg(\n",
    "        tx_count=(\"signature\", \"count\"),\n",
    "        success_count=(\"status\", lambda x: (x == \"success\").sum()),\n",
    "        fail_count=(\"status\", lambda x: (x != \"success\").sum()),\n",
    "        avg_fee=(\"fee\", \"mean\"),\n",
    "        median_fee=(\"fee\", \"median\"),\n",
    "        fee_variance=(\"fee\", \"var\"),\n",
    "        avg_value=(\"value\", \"mean\"),\n",
    "        median_value=(\"value\", \"median\"),\n",
    "        instruction_sum=(\"instructionCount\", \"sum\"),\n",
    "        block_time=(\"blockTime\", \"first\")\n",
    "    ).reset_index()\n",
    "\n",
    "    #Computing block time delta i.e gap between blocks\n",
    "    block_stats = block_stats.sort_values(\"slot\").reset_index(drop=True)\n",
    "    block_stats[\"block_time_delta\"] = block_stats[\"block_time\"].diff().fillna(0)\n",
    "    block_stats[\"block_utilization\"] = block_stats[\"tx_count\"] / block_stats[\"tx_count\"].max()\n",
    "\n",
    "    return block_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "989ff364-de3b-4a75-9946-e72e71516f4e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def build_block_graphs(block_stats, window=16, util_thresh=0.85):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      graphs: list[Data] — each has `window` block-nodes, chain edges within the window\n",
    "      labels: torch.LongTensor — label for the *target* block at the end of each window\n",
    "      target_slots: list[int] — slot numbers aligned to graphs/labels (for tx pairing)\n",
    "    \"\"\"\n",
    "    bs = block_stats.sort_values(\"slot\").reset_index(drop=True).copy()\n",
    "\n",
    "    feat_cols = [\n",
    "        \"tx_count\", \"success_count\", \"fail_count\",\n",
    "        \"avg_fee\", \"median_fee\", \"fee_variance\",\n",
    "        \"block_time_delta\", \"block_utilization\"\n",
    "    ]\n",
    "    X = bs[feat_cols].fillna(0).to_numpy()\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    Xs = scaler.transform(X)\n",
    "\n",
    "    graphs, labels, target_slots = [], [], []\n",
    "\n",
    "    for i in range(window - 1, len(bs)):\n",
    "        #window of blocks ending at i\n",
    "        x_win = torch.tensor(Xs[i - window + 1:i + 1], dtype=torch.float)\n",
    "        n = x_win.size(0)\n",
    "\n",
    "        if n > 1:\n",
    "            src = list(range(n - 1))\n",
    "            dst = list(range(1, n))\n",
    "            edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "        # label from the *target* block (end of window)\n",
    "        y = int(bs.loc[i, \"block_utilization\"] > util_thresh)\n",
    "\n",
    "        # validation\n",
    "        if edge_index.numel() > 0:\n",
    "            assert edge_index.max().item() < n, \\\n",
    "                f\"[BlockGraph] max edge {edge_index.max().item()} >= num_nodes {n}\"\n",
    "\n",
    "        data = Data(x=x_win, edge_index=edge_index)\n",
    "        data.slot = int(bs.loc[i, \"slot\"])  \n",
    "        graphs.append(data)\n",
    "        labels.append(y)\n",
    "        target_slots.append(data.slot)\n",
    "\n",
    "    return graphs, torch.tensor(labels, dtype=torch.long), target_slots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ac67db6c-8d59-41d9-af1b-b8e0dd3dd1a8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def build_tx_graphs(df, target_slots):\n",
    "    \"\"\"\n",
    "    For each slot in `target_slots`, build a tx graph *for that slot only*,\n",
    "    with local 0..n-1 node ids and sequential bidirectional edges.\n",
    "    \"\"\"\n",
    "    df = df.reset_index(drop=True).copy()\n",
    "\n",
    "    feat_cols = [\"fee\", \"value\", \"instructionCount\"]\n",
    "    X = df[feat_cols].fillna(0).to_numpy()\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    Xs = scaler.transform(X)\n",
    "\n",
    "    #Mapping slots to row indices (global)\n",
    "    slot2idxs = df.groupby(\"slot\").indices\n",
    "\n",
    "    graphs = []\n",
    "    for s in target_slots:\n",
    "        idxs = list(slot2idxs.get(s, []))  # all tx rows for this slot (global indices)\n",
    "        n = len(idxs)\n",
    "\n",
    "        if n == 0:\n",
    "            # make a minimal empty-edges graph so batching still works\n",
    "            x = torch.zeros((1, len(feat_cols)), dtype=torch.float)\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        else:\n",
    "            # features *for this slot*, stacked in the slot's order\n",
    "            x = torch.tensor(Xs[idxs], dtype=torch.float)\n",
    "\n",
    "            # IMPORTANT: edges must use LOCAL ids 0..n-1, never the global row ids\n",
    "            if n > 1:\n",
    "                src = list(range(n - 1))\n",
    "                dst = list(range(1, n))\n",
    "                edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n",
    "            else:\n",
    "                edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "        if edge_index.numel() > 0:\n",
    "            assert edge_index.max().item() < x.size(0), \\\n",
    "                f\"[TxGraph] max edge {edge_index.max().item()} >= num_nodes {x.size(0)} for slot {s}\"\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        data.slot = int(s)\n",
    "        graphs.append(data)\n",
    "\n",
    "    return graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9a618eda-5706-4765-8e9b-9350fff41e60",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "\n",
    "# ----- Block Graph Encoder -----\n",
    "class BlockEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(BlockEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = F.relu(self.conv1(x, edge_index))\n",
    "        h = F.relu(self.conv2(h, edge_index))   # <-- added ReLU for stability\n",
    "        return global_mean_pool(h, batch)       # pooled block-level embedding\n",
    "\n",
    "\n",
    "# ----- Transaction Graph Encoder -----\n",
    "class TxEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(TxEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        h = F.relu(self.conv1(x, edge_index))\n",
    "        h = F.relu(self.conv2(h, edge_index))   # <-- added ReLU\n",
    "        return global_mean_pool(h, batch)       # pooled tx-level embedding\n",
    "\n",
    "\n",
    "# ----- Hybrid Congestion Prediction Model -----\n",
    "class HybridCongestionModel(nn.Module):\n",
    "    def __init__(self, block_in, tx_in, hidden=64, out_dim=2):\n",
    "        super(HybridCongestionModel, self).__init__()\n",
    "        # Encoders\n",
    "        self.block_encoder = BlockEncoder(block_in, hidden, hidden)\n",
    "        self.tx_encoder = TxEncoder(tx_in, hidden, hidden)\n",
    "        \n",
    "        # Fusion + classification\n",
    "        self.fc1 = nn.Linear(hidden*2, hidden)\n",
    "        self.dropout = nn.Dropout(0.3)            # regularization\n",
    "        self.fc2 = nn.Linear(hidden, out_dim)     # binary: congested / not congested\n",
    "    \n",
    "    def forward(self, block_data, tx_data):\n",
    "        block_emb = self.block_encoder(block_data.x, block_data.edge_index, block_data.batch)\n",
    "        tx_emb = self.tx_encoder(tx_data.x, tx_data.edge_index, tx_data.batch)\n",
    "        \n",
    "        # Fuse embeddings\n",
    "        h = torch.cat([block_emb, tx_emb], dim=1)  \n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = self.dropout(h)\n",
    "        return self.fc2(h)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "948f9678-aaa7-4ab8-858f-757475681d23",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "\n",
    "def split_dataset(block_graphs, tx_graphs, labels, test_size=0.2):\n",
    "    # Ensure block_graphs[i] matches tx_graphs[i] and labels[i]\n",
    "    idx_train, idx_test = train_test_split(range(len(labels)), test_size=test_size, random_state=42, stratify=None)\n",
    "\n",
    "    train_blocks = [block_graphs[i] for i in idx_train]\n",
    "    test_blocks  = [block_graphs[i] for i in idx_test]\n",
    "    train_txs    = [tx_graphs[i] for i in idx_train]\n",
    "    test_txs     = [tx_graphs[i] for i in idx_test]\n",
    "    y_train      = labels[idx_train]\n",
    "    y_test       = labels[idx_test]\n",
    "\n",
    "    return train_blocks, test_blocks, train_txs, test_txs, y_train, y_test\n",
    "\n",
    "\n",
    "# ----- 2. Custom Dataloader Wrapper -----\n",
    "class HybridDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, block_graphs, tx_graphs, labels):\n",
    "        self.block_graphs = block_graphs\n",
    "        self.tx_graphs = tx_graphs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.block_graphs[idx], self.tx_graphs[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# ----- 3. Collate Function for Hybrid Graphs -----\n",
    "def collate_fn(batch):\n",
    "    block_batch, tx_batch, labels = zip(*batch)\n",
    "    return (\n",
    "        Batch.from_data_list(block_batch),\n",
    "        Batch.from_data_list(tx_batch),\n",
    "        torch.tensor(labels, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "\n",
    "# ----- 4. Training Function -----\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for block_data, tx_data, labels in loader:\n",
    "        block_data, tx_data, labels = block_data.to(device), tx_data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(block_data, tx_data)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = out.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "# ----- 5. Evaluation Function -----\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for block_data, tx_data, labels in loader:\n",
    "            block_data, tx_data, labels = block_data.to(device), tx_data.to(device), labels.to(device)\n",
    "\n",
    "            out = model(block_data, tx_data)\n",
    "            loss = criterion(out, labels)\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "# ----- 6. Main Training Loop -----\n",
    "def run_training(block_graphs, tx_graphs, labels, model, epochs=20, batch_size=32, lr=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Split dataset\n",
    "    train_blocks, test_blocks, train_txs, test_txs, y_train, y_test = split_dataset(block_graphs, tx_graphs, labels)\n",
    "\n",
    "    # Create datasets & loaders\n",
    "    train_dataset = HybridDataset(train_blocks, train_txs, y_train)\n",
    "    test_dataset  = HybridDataset(test_blocks, test_txs, y_test)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Optimizer & Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}: \"\n",
    "              f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n",
    "              f\"Test Loss={test_loss:.4f}, Test Acc={test_acc:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e31d72-3b4c-4f06-acb2-1401a44668b7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b9b43fb3-a0c4-4fbf-9d0e-087e0998c5d4",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n samples (graphs): 81\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Label distribution: {0: 71, 1: 10}\n",
      "train size: 64 test size: 17\n",
      "train label counts: (array([0, 1]), array([56,  8]))\n",
      "test label counts: (array([0, 1]), array([15,  2]))\n",
      "[0] slot=354935319  block_nodes=16 max_e=15 | tx_nodes=1436 max_e=1435\n",
      "[1] slot=354935334  block_nodes=16 max_e=15 | tx_nodes=1427 max_e=1426\n",
      "[2] slot=354935349  block_nodes=16 max_e=15 | tx_nodes=1396 max_e=1395\n",
      "[3] slot=354935365  block_nodes=16 max_e=15 | tx_nodes=1682 max_e=1681\n",
      "[4] slot=365730564  block_nodes=16 max_e=15 | tx_nodes=1278 max_e=1277\n",
      "Epoch 01: Train Loss=0.7325, Train Acc=0.1250, Test Loss=0.7067, Test Acc=0.2353\n",
      "Epoch 02: Train Loss=0.6889, Train Acc=0.5312, Test Loss=0.6631, Test Acc=0.8824\n",
      "Epoch 03: Train Loss=0.6456, Train Acc=0.8750, Test Loss=0.6099, Test Acc=0.8824\n",
      "Epoch 04: Train Loss=0.5771, Train Acc=0.8750, Test Loss=0.5462, Test Acc=0.8824\n",
      "Epoch 05: Train Loss=0.5197, Train Acc=0.8750, Test Loss=0.4771, Test Acc=0.8824\n",
      "Epoch 06: Train Loss=0.4504, Train Acc=0.8750, Test Loss=0.4196, Test Acc=0.8824\n",
      "Epoch 07: Train Loss=0.4176, Train Acc=0.8750, Test Loss=0.3856, Test Acc=0.8824\n",
      "Epoch 08: Train Loss=0.3662, Train Acc=0.8750, Test Loss=0.3816, Test Acc=0.8824\n",
      "Epoch 09: Train Loss=0.3815, Train Acc=0.8750, Test Loss=0.3861, Test Acc=0.8824\n",
      "Epoch 10: Train Loss=0.3936, Train Acc=0.8750, Test Loss=0.3888, Test Acc=0.8824\n",
      "Epoch 11: Train Loss=0.3595, Train Acc=0.8750, Test Loss=0.3867, Test Acc=0.8824\n",
      "Epoch 12: Train Loss=0.3981, Train Acc=0.8750, Test Loss=0.3821, Test Acc=0.8824\n",
      "Epoch 13: Train Loss=0.3942, Train Acc=0.8750, Test Loss=0.3795, Test Acc=0.8824\n",
      "Epoch 14: Train Loss=0.3908, Train Acc=0.8750, Test Loss=0.3787, Test Acc=0.8824\n",
      "Epoch 15: Train Loss=0.3783, Train Acc=0.8750, Test Loss=0.3795, Test Acc=0.8824\n",
      "Epoch 16: Train Loss=0.3729, Train Acc=0.8750, Test Loss=0.3796, Test Acc=0.8824\n",
      "Epoch 17: Train Loss=0.3636, Train Acc=0.8750, Test Loss=0.3797, Test Acc=0.8824\n",
      "Epoch 18: Train Loss=0.3659, Train Acc=0.8750, Test Loss=0.3788, Test Acc=0.8824\n",
      "Epoch 19: Train Loss=0.3747, Train Acc=0.8750, Test Loss=0.3790, Test Acc=0.8824\n",
      "Epoch 20: Train Loss=0.3627, Train Acc=0.8750, Test Loss=0.3801, Test Acc=0.8824\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1) load + aggregate\n",
    "    df = load_transactions()\n",
    "    block_stats = build_block_stats(df)\n",
    "\n",
    "    # 2) build graphs\n",
    "    block_graphs, labels, target_slots = build_block_graphs(block_stats, window=16, util_thresh=0.85)\n",
    "    tx_graphs = build_tx_graphs(df, target_slots)\n",
    "\n",
    "    import numpy as np\n",
    "    print(\"n samples (graphs):\", len(block_graphs))\n",
    "    print(labels)\n",
    "    unique, counts = np.unique(labels.numpy() if hasattr(labels, \"numpy\") else labels, return_counts=True)\n",
    "    print(\"Label distribution:\", dict(zip(unique, counts)))\n",
    "    \n",
    "    # If using train/test split function:\n",
    "    train_blocks, test_blocks, train_txs, test_txs, y_train, y_test = split_dataset(block_graphs, tx_graphs, labels)\n",
    "    print(\"train size:\", len(train_blocks), \"test size:\", len(test_blocks))\n",
    "    print(\"train label counts:\", np.unique(y_train, return_counts=True))\n",
    "    print(\"test label counts:\", np.unique(y_test, return_counts=True))\n",
    "\n",
    "\n",
    "    # 3) quick sanity prints (won’t crash on empty edges)\n",
    "    for i, (b, t) in enumerate(zip(block_graphs[:5], tx_graphs[:5])): \n",
    "        be = (b.edge_index.max().item() if b.edge_index.numel() else -1)\n",
    "        te = (t.edge_index.max().item() if t.edge_index.numel() else -1)\n",
    "        print(f\"[{i}] slot={b.slot}  block_nodes={b.x.size(0)} max_e={be} | \"\n",
    "              f\"tx_nodes={t.x.size(0)} max_e={te}\")\n",
    "\n",
    "    # 4) train\n",
    "    model = HybridCongestionModel(block_in=8, tx_in=3, hidden=64, out_dim=2)\n",
    "    run_training(block_graphs, tx_graphs, labels, model, epochs=20, batch_size=16, lr=1e-3)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b8d0e-81cf-4a43-8bdd-6f63c9b2d728",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc621d8-7f26-4365-bdba-b0a2783d2851",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8215a9-c07f-417f-9b26-29f4daa62e70",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6072420-e6e8-44ae-a246-e436f089ea0b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def build_block_graphs(block_stats, window=16, util_thresh=0.8):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      graphs: list[Data] — each has `window` block-nodes, chain edges within the window\n",
    "      labels: torch.LongTensor — label for the *target* block at the end of each window\n",
    "      target_slots: list[int] — slot numbers aligned to graphs/labels (for tx pairing)\n",
    "    \"\"\"\n",
    "    bs = block_stats.sort_values(\"slot\").reset_index(drop=True).copy()\n",
    "\n",
    "    feat_cols = [\n",
    "        \"tx_count\", \"success_count\", \"fail_count\",\n",
    "        \"avg_fee\", \"median_fee\", \"fee_variance\",\n",
    "        \"block_time_delta\", \"block_utilization\"\n",
    "    ]\n",
    "    X = bs[feat_cols].fillna(0).to_numpy()\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    Xs = scaler.transform(X)\n",
    "\n",
    "    graphs, labels, target_slots = [], [], []\n",
    "\n",
    "    for i in range(window - 1, len(bs)):\n",
    "        # window of blocks ending at i\n",
    "        x_win = torch.tensor(Xs[i - window + 1:i + 1], dtype=torch.float)  # [n, F]\n",
    "        n = x_win.size(0)\n",
    "\n",
    "        if n > 1:\n",
    "            src = list(range(n - 1))\n",
    "            dst = list(range(1, n))\n",
    "            edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "        # label from the *target* block (end of window)\n",
    "        y = int(bs.loc[i, \"block_utilization\"] > util_thresh)\n",
    "\n",
    "        # validation\n",
    "        if edge_index.numel() > 0:\n",
    "            assert edge_index.max().item() < n, \\\n",
    "                f\"[BlockGraph] max edge {edge_index.max().item()} >= num_nodes {n}\"\n",
    "\n",
    "        data = Data(x=x_win, edge_index=edge_index)\n",
    "        data.slot = int(bs.loc[i, \"slot\"])  # keep slot to align with tx graphs later\n",
    "        graphs.append(data)\n",
    "        labels.append(y)\n",
    "        target_slots.append(data.slot)\n",
    "\n",
    "    return graphs, torch.tensor(labels, dtype=torch.long), target_slots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5990189-1083-4902-8834-d3973c4c994b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def build_tx_graphs(df, target_slots):\n",
    "    \"\"\"\n",
    "    For each slot in `target_slots`, build a tx graph *for that slot only*,\n",
    "    with local 0..n-1 node ids and sequential bidirectional edges.\n",
    "    \"\"\"\n",
    "    df = df.reset_index(drop=True).copy()\n",
    "\n",
    "    feat_cols = [\"fee\", \"value\", \"instructionCount\"]\n",
    "    X = df[feat_cols].fillna(0).to_numpy()\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    Xs = scaler.transform(X)\n",
    "\n",
    "    # Map slot -> row indices (global)\n",
    "    slot2idxs = df.groupby(\"slot\").indices\n",
    "\n",
    "    graphs = []\n",
    "    for s in target_slots:\n",
    "        idxs = list(slot2idxs.get(s, []))  # all tx rows for this slot (global indices)\n",
    "        n = len(idxs)\n",
    "\n",
    "        if n == 0:\n",
    "            # make a minimal empty-edges graph so batching still works\n",
    "            x = torch.zeros((1, len(feat_cols)), dtype=torch.float)\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        else:\n",
    "            # features *for this slot*, stacked in the slot's order\n",
    "            x = torch.tensor(Xs[idxs], dtype=torch.float)\n",
    "\n",
    "            # IMPORTANT: edges must use LOCAL ids 0..n-1, never the global row ids\n",
    "            if n > 1:\n",
    "                src = list(range(n - 1))\n",
    "                dst = list(range(1, n))\n",
    "                edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)\n",
    "            else:\n",
    "                edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "        if edge_index.numel() > 0:\n",
    "            assert edge_index.max().item() < x.size(0), \\\n",
    "                f\"[TxGraph] max edge {edge_index.max().item()} >= num_nodes {x.size(0)} for slot {s}\"\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        data.slot = int(s)\n",
    "        graphs.append(data)\n",
    "\n",
    "    return graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97855407-927c-4869-a044-51d36285c2d8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] slot=354935319  block_nodes=16 max_e=15 | tx_nodes=1436 max_e=1435\n",
      "[1] slot=354935334  block_nodes=16 max_e=15 | tx_nodes=1427 max_e=1426\n",
      "[2] slot=354935349  block_nodes=16 max_e=15 | tx_nodes=1396 max_e=1395\n",
      "[3] slot=354935365  block_nodes=16 max_e=15 | tx_nodes=1682 max_e=1681\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     run_training(block_graphs, tx_graphs, labels, model, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[32], line 19\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 4) train\u001b[39;00m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m HybridCongestionModel(block_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, tx_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, out_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m run_training(block_graphs, tx_graphs, labels, model, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 92\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(block_graphs, tx_graphs, labels, model, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     89\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Split dataset\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m train_blocks, test_blocks, train_txs, test_txs, y_train, y_test \u001b[38;5;241m=\u001b[39m split_dataset(block_graphs, tx_graphs, labels)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Create datasets & loaders\u001b[39;00m\n\u001b[1;32m     95\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m HybridDataset(train_blocks, train_txs, y_train)\n",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m, in \u001b[0;36msplit_dataset\u001b[0;34m(block_graphs, tx_graphs, labels, test_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_dataset\u001b[39m(block_graphs, tx_graphs, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Ensure block_graphs[i] matches tx_graphs[i] and labels[i]\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     idx_train, idx_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(labels)), test_size\u001b[38;5;241m=\u001b[39mtest_size, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[1;32m     14\u001b[0m     train_blocks \u001b[38;5;241m=\u001b[39m [block_graphs[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx_train]\n\u001b[1;32m     15\u001b[0m     test_blocks  \u001b[38;5;241m=\u001b[39m [block_graphs[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx_test]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2801\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2797\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2799\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2801\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[1;32m   2803\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[1;32m   2805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2806\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2807\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2808\u001b[0m     )\n\u001b[1;32m   2809\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:1843\u001b[0m, in \u001b[0;36mBaseShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   1814\u001b[0m \n\u001b[1;32m   1815\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1840\u001b[0m \u001b[38;5;124;03mto an integer.\u001b[39;00m\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1842\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m-> 1843\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_indices(X, y, groups):\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2247\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit._iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2245\u001b[0m class_counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(y_indices)\n\u001b[1;32m   2246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmin(class_counts) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 2247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe least populated class in y has only 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m member, which is too few. The minimum\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2250\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m number of groups for any class cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be less than 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m<\u001b[39m n_classes:\n\u001b[1;32m   2255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be greater or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n_train, n_classes)\n\u001b[1;32m   2258\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # 1) load + aggregate\n",
    "    df = load_transactions()\n",
    "    block_stats = build_block_stats(df)\n",
    "\n",
    "    # 2) build graphs\n",
    "    block_graphs, labels, target_slots = build_block_graphs(block_stats, window=16, util_thresh=0.8)\n",
    "    tx_graphs = build_tx_graphs(df, target_slots)\n",
    "\n",
    "    # 3) quick sanity prints (won’t crash on empty edges)\n",
    "    for i, (b, t) in enumerate(zip(block_graphs[:5], tx_graphs[:5])):  # first few\n",
    "        be = (b.edge_index.max().item() if b.edge_index.numel() else -1)\n",
    "        te = (t.edge_index.max().item() if t.edge_index.numel() else -1)\n",
    "        print(f\"[{i}] slot={b.slot}  block_nodes={b.x.size(0)} max_e={be} | \"\n",
    "              f\"tx_nodes={t.x.size(0)} max_e={te}\")\n",
    "\n",
    "    # 4) train\n",
    "    model = HybridCongestionModel(block_in=8, tx_in=3, hidden=64, out_dim=2)\n",
    "    run_training(block_graphs, tx_graphs, labels, model, epochs=20, batch_size=16, lr=1e-3)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fcb613-96cd-4ede-8df9-c02e61a27ccb",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e9f3cb-6dbc-4c29-abb0-634c4ddcc837",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bdee971-8115-4d0f-b75e-676da908b65f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def load_and_aggregate(mongo_uri=\"mongodb://localhost:27017\", db_name=\"solana_tx_db\", collection_name=\"transactions\"):\n",
    "    client = MongoClient(mongo_uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "    df = pd.DataFrame(list(collection.find({})))\n",
    "    \n",
    "    block_stats = df.groupby(\"slot\").agg(\n",
    "        block_time=(\"blockTime\", \"first\"),\n",
    "        tx_count=(\"signature\", \"count\"),\n",
    "        success_count=(\"status\", lambda x: (x == \"success\").sum()),\n",
    "        fail_count=(\"status\", lambda x: (x != \"success\").sum()),\n",
    "        avg_fee=(\"fee\", \"mean\"),\n",
    "        median_fee=(\"fee\", \"median\"),\n",
    "        fee_variance=(\"fee\", \"var\"),\n",
    "        programs=(\"programs\", lambda x: x.explode().value_counts().to_dict())\n",
    "    ).reset_index()\n",
    "    \n",
    "\n",
    "    block_stats[\"block_time_delta\"] = block_stats[\"block_time\"].diff()\n",
    "    MAX_TX_PER_BLOCK = 5000\n",
    "    block_stats[\"block_utilization\"] = block_stats[\"tx_count\"] / MAX_TX_PER_BLOCK\n",
    "    \n",
    "    return block_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40923e3e-9104-4034-8168-0df228e55968",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def build_graph(block_stats):\n",
    "    features = block_stats[[\n",
    "        \"tx_count\", \"success_count\", \"fail_count\",\n",
    "        \"avg_fee\", \"median_fee\", \"fee_variance\",\n",
    "        \"block_time_delta\", \"block_utilization\"\n",
    "    ]].fillna(0)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    x = torch.tensor(scaler.fit_transform(features), dtype=torch.float)\n",
    "    \n",
    "\n",
    "    src = list(range(len(block_stats)-1))\n",
    "    dst = list(range(1, len(block_stats)))\n",
    "    edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long) \n",
    "    \n",
    "    # Target label is to predict congestion (binary classification)\n",
    "    # Define congestion = utilization > 0.8\n",
    "    y = torch.tensor((block_stats[\"block_utilization\"] > 0.8).astype(int).values, dtype=torch.long)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, y=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7761ab2d-1d45-47db-bed8-bcd62f1a9e61",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class BlockCongestionGCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=32, output_dim=2):\n",
    "        super(BlockCongestionGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f222cbb-adce-4e7a-9362-5891705c3f5b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def train_gnn(data, epochs=50, lr=0.01):\n",
    "    model = BlockCongestionGCN(input_dim=data.num_node_features)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            pred = out.argmax(dim=1)\n",
    "            acc = (pred == data.y).sum().item() / data.num_nodes\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, Acc: {acc:.4f}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47fd3992-b1cd-4b94-9148-a1178f597f54",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7340, Acc: 0.3684\n",
      "Epoch 10, Loss: 0.0087, Acc: 1.0000\n",
      "Epoch 20, Loss: 0.0000, Acc: 1.0000\n",
      "Epoch 30, Loss: 0.0000, Acc: 1.0000\n",
      "Epoch 40, Loss: 0.0000, Acc: 1.0000\n",
      "Epoch 50, Loss: 0.0000, Acc: 1.0000\n",
      "Epoch 60, Loss: 0.0000, Acc: 1.0000\n",
      "Epoch 70, Loss: 0.0000, Acc: 1.0000\n",
      "Epoch 80, Loss: 0.0000, Acc: 1.0000\n",
      "Epoch 90, Loss: 0.0000, Acc: 1.0000\n",
      "         slot  block_utilization  predicted_congestion\n",
      "0   354935068             0.3392                     0\n",
      "1   354935084             0.3352                     0\n",
      "2   354935100             0.3162                     0\n",
      "3   354935116             0.3332                     0\n",
      "4   354935132             0.3494                     0\n",
      "5   354935148             0.3244                     0\n",
      "6   354935164             0.3268                     0\n",
      "7   354935183             0.2964                     0\n",
      "8   354935199             0.2948                     0\n",
      "9   354935215             0.3172                     0\n",
      "10  354935232             0.2852                     0\n",
      "11  354935248             0.3248                     0\n",
      "12  354935267             0.3346                     0\n",
      "13  354935284             0.3634                     0\n",
      "14  354935303             0.2948                     0\n",
      "15  354935319             0.2872                     0\n",
      "16  354935334             0.2854                     0\n",
      "17  354935349             0.2792                     0\n",
      "18  354935365             0.3364                     0\n"
     ]
    }
   ],
   "source": [
    "#load mongodb data, build graph and training gnn model\n",
    "block_stats = load_and_aggregate()\n",
    "graph_data = build_graph(block_stats)\n",
    "model = train_gnn(graph_data, epochs=100)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(graph_data)\n",
    "    predictions = out.argmax(dim=1).numpy()\n",
    "\n",
    "block_stats[\"predicted_congestion\"] = predictions\n",
    "print(block_stats[[\"slot\", \"block_utilization\", \"predicted_congestion\"]].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f16baa-3fd1-475f-a9be-282daf226ffe",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
